**Datawarehousing in Google Cloud**

**BigQuery-Datawarehouse**

- Exabyte scale modern Datawarehousing solution from GCP.
  - Relational database(SQL, Schema, Consistency etc)
    - Use SQL-like commands to query massive datasets
  - Traditional (Storage+Compute)+Modern(Realtime+Serverless)
- When we are talking about a Datawarehouse, importing and exporting data (and formats) becomes very important:
  - Load data from a variety of sources, including streaming data
    - Variety of import formats-CSV, JSON, Avro, Parquet, ORC, Datastore Backup
  - Exports to Cloud Storage (long term storage) & Data Studio (Visualization)
    - Formats: CSV/JSON (with Gzip compression), Avro (with deflate or snappy compression)
- Automatically expire data (Configure Table Expiration)
- Query external data sources without storing data in BigQuery
  - Cloud Storage, Cloud SQL, BigTable, Google Drive
  - Use permanent or temporary external tables
  - BigQuery mixes both modern and traditional approach and it can stream and run huge queries.

**BigQuery-Accessing and Querying Data**

- Access Database using:
  - Cloud Console
  - bq command-line tool (NOT gcloud)
  - BigQuery Rest API OR
  - HBase API based libraries (Java,.NET & Python)
- (Remember) BigQuery queries can be expensive as you are running them on large data sets!
- (Best Practice) Estimate BigQuery queries before running:
  - Use UI(console) bq(--dry-run)- Get scanned data volume (estimate)
  - Use Pricing Calculator: Find price for scanning 1 MB data. Calculate cost.

**Partitioning and Clustering BIgQuery Tables- Use Case**

- You pay for BigQuery queries by the amount of data scanned.
  - For Instance, if we scan 20000 rows and the query returns 5000 rows, we need to pay for the 20000 rows.
- Scenario: Imagine a Questions table with millions of rows
  - You want to find all questions asked between a date range (date between 2022-10-02 and 2028-10-02) belonging to a specific category.
    - If you have a single questions table you need to scan all the rows
      - Partitioning- Divide table into multiple segments (example: by date). Partitions between the dates are only scanned.
      - Clustering- Group related data (example: by category)
- Partitioning: Table is divided into segments
  - This process makes it easy to manage and query the data.
  - Improve performance and reduce costs.
  - Partition based on Ingestion time (arrival time) OR a column (TIMESTAMP,DATE,or DATETIME, or INTEGER).
  - (Default) All partitions will share same schema as table.
  - Allows you to expire (delete) parts of table data easily (partition_expiration_days).
- Clustering: Organize table data based on the contents of one or more columns
  - Goal: colocate related data and eliminate scans of unnecessary data.
  - Avoid Creating too many small partitions (of less than 1 GB). In those cases, prefer clustering.

**Partitioning and Clustering BigQuery Tables- Syntax**

```
CREATE TABLE `my_data_set.questions_partitioned_and_clustered`
...
...
PARTITIONED BY
    DATE(created_date)
    CLUSTER_BY_category
...
OPTIONS  (
  expiration_timestamp=TIMESTAMP "2015-01-01 00:00:00 UTC",
  partition_expiration_days=7
)
```

**Expiring Data in BigQuery**

```
CREATE SCHEMA mydataset
OPTIONS(
  default_table_expiration_days=3.75
)

ALTER TABLE mydataset.mytable
SET OPTIONS(
  expiration_timestamp=TIMESTAMP "2025-01-01 00:00:00 UTC",
  partition_expiration_days=7
)
```

- You pay for data stored in BigQuery:
  - How can you automatically delete (expire) data which is not needed?
- Big Query Hierarchy: Data Set > Table > Partitions
  - You can configure expiration at each level
    - Configure default table expiration (default_table_expiration_days) for datasets
    - Configure expiration time (expiration_timestamp) for tables
    - Configure partition expiration (partition_expiration_days) for partitioned tables
- Best Practice: Expire Tables and Partitions you are not using!

**Importing Data into BigQuery**

```
Batch              ---->Big Query
Streaming          ---->Big Query
Federation         ---->Big Query
DataTransferService---->Big Query
```

- Batch Import (Free):
  - Import from Cloud Storage and local files
  - Import after processing by Cloud Dataflow and Cloud Dataproc
- Streaming Import ($$$$):
  - From Cloud Pub/Sub, Streaming Inserts
  - Import after processing by Cloud Dataflow and Cloud Dataproc
- Federated Queries: Query external data
  - Cloud Storage, Cloud SQL, BigTable, Google Drive
- BigQuery Data Transfer Service: Import from
  - Google SaaS apps (Google Ads, Cloud Storage etc)
  - External cloud storage providers- Amazon S3
  - Data warehouses- Teradata, Amazon Redshift

**Streaming Data into BigQuery**

- Loading data in bulk is free but streaming data is NOT FREE
  - AND there are a lot of limitations(Use with caution!)
- Streaming Data can contain duplicates. How can you avoid duplicates?
  - Add *insertId* with each streaming insert:
    - insertId is used to provides best effort de-duplication (for up to one minute)
      - for strict de-duplication and transactions, try Google Cloud Datastore
- There are strict streaming quotas with BigQuery:
  - IF you are NOT populating insertId:
    - Maximum bytes per second: 1 GB per second, per project (Remember per project- NOT per Table)
  - ELSE (i.e. you are using insertId)
    - Maximum rows per second per project
      - US and EU multi-regions: 5,00,000, Other locations: 1,00,000
      - per table limitation: 1,00,000
    - Maximum bytes per second: 100 MB
- If you have streaming of millions of rows per second, prefer BigTable!

**Best Practices: BigQuery**

- Estimate your queries before running them
  - bq --dry_run flag or dryRun API parameter
- Use clustering and partitioning for your tables
- Avoid streaming inserts when possible
  - Loading data in bulk is free but streaming data is NOT FREE
  - Offers Best effort de-duplication (when you use insertId)
  - Remember Quota limits
- Expire Data Automatically:
  - Configure default table expiration (default_table_expiration_days) for datasets
  - Configure expiration time for tables
  - Configure partition expiration for partitioned tables
- Consider Long-Term storage option:
  - Long-term storage: Table in which data is NOT edited for 90 consecutive days
  - Low Storage Cost: Similar to Cloud Storage Nearline
- BigQuery is fast for complex queries: 
  - BUT it is not as well optimized for narrow-range queries (Prefer Cloud BigTable)
  - (Remember) Too much complexity in setting up a query.
- Optimize BigQuery Usage using Audit logs:
  - Analyze queries/jobs that were run earlier
  - Stream your audit logs (BigQueryAuditMetadata) to BigQuery
    - Understand Usage Patterns (query cost by user)
    - Optimize (visualize using Google Data Studio)

**Cloud Dataproc**

- Managed spark and hadoop service:
  - Variety of jobs are supported:
    - Spark, PySpark, SparkR, Hive, SparkSQL, Pig, Hadoop
  - Perform complex batdch processing
- Multiple Cluster Modes:
  - Single Node/ Standard/ High Availability (3 Masters)
  - Use Regular/ preemptible VMs
- Use Case: Move your Hadoop and Spark Clusters to the cloud.
  - Perform your machine learning and AI development using open source frameworks
- (Remember) Cloud Dataproc is a data analysis platform
  - You can export cluster configuration but NOT data
- (Alternative) BigQuery- When you run SQL queries on Petabytes
  - Go for Cloud Dataproc when you need more than queries (Example: Complex Batch processing Machine Learning and AI workloads)

**Notes**

- *Partitioning, Clustering and Configure Table and Partition Expiration* are the options you can consider to reduce the costs of your BigQuery jobs.
- *Partitioning*, helps you divide a table into multiple segments based on the ingestion date.
- *Clustering*, helps you co-locate related data in a BigQuery table and eliminate scans of unnecessary data.
- *BigQuery Data Transfer Service*, would you recommend to import data from Amazon S3, Amazon Redshift or an on-premise Teradata installation into Bigquery.
- *Cloud Dataproc*, is recommended for running complex machine learning and AI workloads on your Hadoop and Spark Clusters and You want to move these workloads to Google Cloud.