**Data Lifecyle and Architecture**

**Data Lifecycle**

```
1. Ingest   --->    2. Store    --->    3. Process and Analyze  --->    4. Explore and Visualize
```

- Four Steps:
  - Ingest: Stream or Batch Ingest
  - Store: Durably and cost efficiently store data in a convinient format.
  - Process and Analyze: Convert data to information (normalizations or aggregations)
  - Explore and Visualize: Flexibility to play with data/information. Get and share insights.

**Data Lifecycle-1-Ingest**

- Streaming: Pub/Sub
- Batch: Storage Transfer Service, BigQuery Transfer Service, Transfer Appliance, gsutil etc
- Database Migration: Migrate data from other sources to Google Cloud.
  - Database Migration Service (Simplifying migrations to cloud SQL)
  - Batch transfer to cloud storage
  - Load data into database from cloud storage using dataflow.

**Data Lifecycle-2-Store**

<table>
<thead>
<tr>
<th>Service</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cloud Storage</td>
<td>Object Storage(unstructured data)</td>
</tr>
<tr>
<td>Cloud SQL</td>
<td>Managed MySQL, Postgre SQL and MS SQL Server databases Relational, pre-defined schema, strong transactions, regional</td>
</tr>
<tr>
<td>Cloud Spanner</td>
<td>Horizontally scalable relational database Relational, pre-defined schema, strong transactions, high availability and global scale</td>
</tr>
<tr>
<td>Cloud Firestore</td>
<td>Flexible, Scalable, Transactional NOSQL database</td>
</tr>
<tr>
<td>Cloud Bigtable</td>
<td>Managed wide-column NoSQL Petabyte Scale, Real-Time apps and large-scale analytical time-series workloads, single-row transactions</td>
</tr>
<tr>
<td>Bigquery</td>
<td>Managed Data Warehouse</td>
</tr>
<tr>
<td>Custom Database</td>
<td>Use Cloud Marketplace to deploy an open source database of your choice- MongoDN, Cassandra etc.</td>
</tr>
</tbody>
</table>

**Data Lifecycle-3-Process and Analyze**

```
Raw Data    --->    Actionable Information(Clean, Transform)
```

<table>
<thead>
<tr>
<th>Service</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataprep</td>
<td>Clean and Prepare Data, Fully Managed, No-Ops. Usecases: Clean Data on-boarded from external sources, Prepare data for ML Visual approach for non programmers</td>
</tr>
<tr>
<td>Cloud Data Loss Prevention</td>
<td>Scan, Discover, classify and report on data in Cloud Storage, BigQuery, and Datastore (mask, tokenize and transform sensitive elements)</td>
</tr>
<tr>
<td>Dataflow</td>
<td>More flexible ETL pipelines (Fully Managed, No-Ops, Batch and Streaming)</td>
</tr>
<tr>
<td>Dataproc</td>
<td>Complex Processing using Spark and Hadoop. Needs a cluster with Compute Engine VMs. UseCases: Machine Learning, Migrate Existing spark and Hadoop workloads.</td>
</tr>
</tbody>
</table>

**Data Lifecycle-4-Explore and Visualize**

<table>
<thead>
<tr>
<th>Service</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cloud BigQuery</td>
<td>Managed Data Warehouse, Standard SQL, Serverless, Separate Storage and Compute</td>
</tr>
<tr>
<td>ML-Pre built models</td>
<td>Vision API, Speech-to-text, Natural Language API, Video Intelligence API etc.</td>
</tr>
<tr>
<td>ML-Custom models</td>
<td>Use AI Platform (based on TensorFlow). Use Dataflow for pre-processing.</td>
</tr>
<tr>
<td>Cloud Datalab</td>
<td>Web based tool to explore, analyze and visualize data. Based on Jupyter notebooks (Use Python, SQL Queries etc). Support for popular data-science toolkits-pandas, numpy, and scikit-learn.</td>
</tr>
<tr>
<td>Cloud Data Studio</td>
<td>Dashboard and Visualization. Live charts and graphs based on data in cloud SQL, BigQuery etc.</td>
</tr>
<tr>
<td>Cloud Data Catalog</td>
<td>Data discovery and metadata management. Unified view of all datasets. Tag sensitive data using Cloud Data Loss Prevention (DLP).</td>
</tr>
</tbody>
</table>

**Big Data and Analytics in GCP**

<table>
<thead>
<tr>
<th>Service</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pub/Sub</td>
<td>Foundation for stream analytics and event-driven systems.</td>
</tr>
<tr>
<td>BigQuery</td>
<td>Serverless data warehouse to analyze petabytes of data. Scale storage and compute separately.</td>
</tr>
<tr>
<td>Google Data Studio</td>
<td>Managed visual analytics service.</td>
</tr>
<tr>
<td>Dataflow</td>
<td>Data pipelines for (Stream + Batch) use cases.</td>
</tr>
<tr>
<td>Dataproc</td>
<td>Managed Apache Spark and Apache Hadoop clusters</td>
</tr>
<tr>
<td>Dataprep</td>
<td>Clean and prepare data (Structured and Unstructured)</td>
</tr>
<tr>
<td>Datalab</td>
<td>Explore, analyze and visualize data on Jupyter notebooks (Use Python, SQL Queries etc) Integrates well with BigQuery.</td>
</tr>
<tr>
<td>Cloud Composer</td>
<td>Managed Workflow orchestration service. Create pipelines across clouds and on-premises data centers.</td>
</tr>
</tbody>
</table>

**Big Data Flow - Batch Ingest into BigQuery**

- Use extract, transform, and load (ETL) to load data into BigQuery.
  - Dataprep: Clean and Prepare Data
  - Dataflow: Create data pipelines (and ETL)
  - Dataproc: Complex processing using Spark and Hadoop

**Streaming Workflow - Enable Realtime Querying**

- Query data in Realtime:
  - Pub/Sub and Dataflow: Analyze, aggregate and filter data before storing to BigQuery.
  - For pre-defined time series analytics, storing data in bigtable gives you the ability to perform rapid analysis.
  - For ad-hoc complex analysis, prefer BigQuery.

**IOT**

- IOT Core: Manage IOT (Registration, Authentication and Authorization) devices.
  - Send/Receive Messages/Real-Time telemetry from/to IOT devices
- Pub/Sub: Durable message ingestion service (allows buffering)
- Dataflow: Processing data (ETL & More)
  - Alternative: Use cloud functions to trigger alerts.
- Data Storage and Analytics:
  - Make IOT data available to mobile or web apps => Datastore
  - Execute pre-defined time series queries => Bigtable
  - More complex or ad hoc analytics/analysis => BigQuery

**Data Lake - Simplified Big Data Solutions**

- Usual big data solutions are complex.
- How can we make collecting, analyzing (reporting, analytics, machine learning) and visualizing huge datasets easy.
- How do design solutions that scale?
- How to build flexibility while saving cost?
- Data Lake
  - Single platform with combination of solutions for data storage, data management and data analytics.

**GCP Data Lakes - Storage and Ingestion**

- Storage: Cloud Storage (low cost + durability + performance + flexile processing)
- Data Ingestion:
  - Streaming data - Cloud Pub/Sub + Cloud Dataflow
  - Batch - Transfer Service + Transfer Appliance + gsutil
- Processing and Analytics
  - Run in-place querying using sql queries using bigquery or (Hive on dstaproc)
- Data Mining and Exploration
  - Clean and Transform raw data with dataprep
  - Use cloud datalab (data science libraries such as Tensorflow and Numpy) for exploring.

**NOTES**

- Cloud Dataprep can be used to clean data on-boarded from external sources.
- Cloud Data Loss Prevention can be used to mask, tokenize and transform sensitive elements in your data stored in cloud storage, bigquery and datastore.
- Cloud Dataflow is a service that can be used to build flexible batch and streaming pipelines.
- Cloud Datalab is a service enable you to run jupyter notebooks to explore, analyze and visualize your data running Python programs and SQL queries.
- Cloud Data Studio is a service that can be used to create dashboards and visualization around data stored in BigQuery.
- Pub/Sub > Dataflow > BigQuery : This represents a simple architecture to execute real-time simple pre-defined queries on a single table containing huge volumes of time series data that is continuously streamed in.
- Cloud Storage is a service that is used as storage for Data lake in GCP.