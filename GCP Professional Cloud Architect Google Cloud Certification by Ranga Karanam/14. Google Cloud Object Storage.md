**Google Cloud Object Storage**

- The name of bucket is globally unique and a permanent name.
- While choosing where to store your data, you can select the location where you want to store your data.
  - These permanent choices defines the geographic placement of your data and affects the cost, performance, and availability of your data.
    - LocatioN Types:
      - Regional : Lowest latency within a single region.
      - Dual-Region : High-availability and low latency across two regions.
      - Multi-Region : Highest availability across largest area.
- Choosing a default storage class for your data:
  - Standard : Best for short-term storage and frequently accessed data.
  - Nearline : Best for backups and data accessed less than once a month.
  - Coldline : Best for disaster recovery and data accessed less than once a quarter.
  - Archive : Best for long term digital preservation of data accessed less than once a year.
- Choosing how to control access to objects:
  - Fine-grained access control : Specify access to individual objects by using object-level permissions and access control lists (ACLs) in addition to bucket-level permissions(IAM).
  - Uniform access control : Ensure uniform access to all objects in the bucket by using only bucket level permissions(IAM). This option becomes permanent after 90 days.
- Now you can create your bucket.

**Cloud Storage**

- Most popular, very flexible and inexpensive storage service.
  - Serverless: Autoscaling and Infinite storage.
- Storage large objects as using a key-value approach:
  - Treats entire object as a unit (partial updates are not allowed).
    - Recommended when you operate on entire object most of the time.
    - Access control at object level.
  - Also called Object Storage. You're using every single file as an object and treating that whole file as a singke unit.
- Provides REST api to access and modify the objects.
  - Provides CLI (gsutil) to access and modify the objects and client libraries for many languages like C++, C#, Java, Node.js, PHP, Python and Ruby.
- *Store all file types*: Text, binary, backup and archives:
  - Media files and archives, Application packages and logs.
  - Backups of your databases or storage devices.
  - Staging data during on-premise to cloud database migration.
  - cloud storage is a multi purpose object storage option in GCP.

**Cloud Storage- Objects and Buckets**

- Objects are stored in buckets.
  - Bucket names are globally unique
  - Bucket names are used as part of the object URLs=> Can contain only lowercase letters, numbers, hyphens, underscores, and periods.
  - 3-63 characters max, Can't start with goog perfix or should not contain google (even misspelled).
  - You can have and store unlimited objects in a bucket.
  - Each object is defined by a unique key.
    - Key is unique in a bucket.
  - Max object size is 5TB.
    - But you can store unlimited number of such objects.

**Cloud Storage- Storage Classes**

- Different kinds of data can be stored in cloud storage.
  - Media files and archives
  - Application packages and logs
  - Backups of your databases or storage devices
  - Long term archives
- Huge variation in access patterns.
- Storage classes help to optimize your costs based on your access needs.
  - Designed for durability of 99.999999999%(11 9's).

<table>
<thead>
<tr>
<th>Storage Class</th>
<th>Name</th>
<th>Minimum Storage Duration</th>
<th>Typically Monthly Availability</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standard</td>
<td>STANDARD</td>
<td>None</td>
<td>Greater than 99.99% in multi and dual region, 99.99% in regions</td>
<td>Frequently used data/short period of time.</td>
</tr>
<tr>
<td>Nearline</td>
<td>NEARLINE</td>
<td>30 days</td>
<td>99.95% in multi-region and dual region, 99.99% in regions</td>
<td>Read or modify once a month on average.</td>
</tr>
<tr>
<td>Coldline Storage</td>
<td>COLDLINE</td>
<td>90 days</td>
<td>99.95% in multi-region and dual region, 99.99% in regions</td>
<td>Read and modify at most once a quarter.</td>
</tr>
<tr>
<td>Archive Storage</td>
<td>ARCHIVE</td>
<td>365 days</td>
<td>99.95% in multi-region and dual region, 99.99% in regions</td>
<td>Less than once a year.</td>
</tr>
</tbody>
</table>

- Features across storage classes:
  - High durability (99.999999999%) annual durability
  - Low latency (first byte typically in tens of milliseconds)
  - Unlimited storage
    - Autoscaling (no configuration needed)
    - No minimum object size
  - Same APIs across storage classes.
  - Committed SLA is 99.95% for multi-region or dual-region location of cloud storage and 99.9% for nearline, coldline or archive storage class in a multi-region or dual-region location or 99.0% for nearline, coldline or archive storage class in a regional location of cloud storage.

**Cloud Storage- Object Versioning**

- Prevents accidental deletion and provides history of objects.
  - Versioning is enabled at the bucket level.
    - This feature can be turned on or off at any time.
  - Live Version is the latest version of this feature.
    - If you delete live object, it becomes noncurrent object version.
    - If you delete noncurrent object version, it will be deleted.
  - Older versions are uniquely identified by (Object Key + a generation number)
  - Reduce the costs by deleting older (noncurrent) versions of objects.

**Cloud Storage- Object Lifecycle Management**

- Lifecycle rules let you apply actions to a bucket's object when certain conditions are met, for example, switching objects to colder storage classes when they reach or pass certain age.
- Files are frequently accessed when they are created.
  - Generally usage reduces with time.
  - How do you save costs by moving files automatically between storage classes?
    - Solution: Object Lifecycle management
  - Objects identification can be done using conditions based on:
    - Age, CreatedBefore, IsLive, MatchesStorageClass, NumberofNewerVersions, etc.
    - Set multiple conditions: all conditions must be satisfied for the action to happen
  - Two kinds of actions:
    - SetStorageClass actions (changes from one storage class to another) (Like change the object class from standard to archive if it is more than 1 year old.)
    - Deletion action (Like, If age of object is greater than 1 year delete the object.)
  - Allowed transitions:
    - (Standard or multi-regional or regional) to (nearline or coldline or archive)
    - Nearline to (coldline or archive)
    - Coldline to archive

Object Lifecycle management- Example rule:
 
 {

  "lifecycle": {

    "rule":[

      {

        "action": {"type" : "Delete"},

        "condition": {

          "age": 30,

          "isLive": true

        }

      },

      {

        "action": {

          "type": "SetStorageClass",

          "storageClass": "NEARLINE"

        },

        "condition": {

          "age": 365,

          "matchesStorageClass": ["MULTI_REGIONAL","STANDARD","DURABLE_REDUCED_AVAILABILITY]

        }

      }

    ]

  }

 }


**Cloud Storage: Encryption**

- Cloud Storage encrpyts data on the server side by default. It's like encrypt and store the data.
- If user is encrypting and sending the file to cloud storage, it is called as Client side encryption.
- Server-Side encryption: Performed by GCS after it receives the data or If you're depending on cloud storage for encrypting your files, that is called as Server side encryption.
  - Google-managed: Default (No configuration needed)
  - Customer-managed: Keys managed by customer in Cloud KMS. You can create the key pair and can use it to encrypt the data on your and server end. You can configure this encryption while creating a bucket in advanced settings by choosing the customer managed encryption key (CMEK).These keys are managed by customer in cloud KMS.
    - GCS service account should have access to customer managed keys in KMS to be able to encrypt and decrypt the files.
  - Customer-supplied: Customer supplies the keys with every GCS operation. 
    - When you use this option the cloud storage will not store the key.It will just use the key to encrypt the data but will not store the key. Whenever you want to decrypt the data you have to provide the key. This is useful when you want to encrypt the data and store it in some other place. Customer is responsible for storing and using it when making API calls. 
      - Use API headers when making API calls.
        - x-goog-encryption-algorithm, x-goog-encryption-key(base 64 encryption key), x-goog-encryption-key-sha256 (encryption key hash)
      - or when using gsutil: in boto configuration file, configure encryption_key under GSUtil section.
- (Optional) Client Side Encryption: Encryption performed by customer before upload.
  - GCP does not know about the keys used in client side encryption.
  - Here GCP is not involved in encryption or decryption process.

**Cloud Storage: Metadata**

- Each object in cloud storage can have Metadata associated with it.
  - Key Value Pairs ex:storageClass:STARNDARD
    - Storage class of an object is represented by metadata.
  - Fixed-key metadata: Fixed key- Changing Value
    - Cache-Control: public, max-age=3600 (Is caching alowed? How long should it be cached?)
    - Content-Disposition: attachment; filename="myfile.pdf" (Should content be displayed inline in the browser or should it be an attachment, which can be downloaded?)
    - Content-tyoe: application/pdf (what kind of content does the object have?)
  - Custom metadata: You can define your own keys and values.
  - Non-editable metadata: You cannot edit these things directly.
    - Storage class of the object, customer-managed encryption key, etc.

**Cloud Storage Bucket Lock- Meet Compliance Requirements**

- It is to ensure that you comply with regulatory and compliance requirements around immutable storage in a Cloud Storage bucket.
- Configure data retention policy with retention period:
  - How long should objects in the bucket be retained for?
    - "Objects in the bucket can only be deleted or replaced once their age is greater than the retention period."
- Retention Policy: Prevents the deletion or modification of the buckets objects for a specified minimum period of time after they're uploaded. The optional step of locking a retention policy ensures that no one (including you) can shorten or remove the retention period.
  - This retention policy can be locked and even unlocked. You can also delete the retention policy.
  - If you switch on the retention policy, you'll incur storage costs for the full retention period of the bucket's objects.
- You can set it while creating a bucket or at a later point in time.
  - Applies automatically to existing objects in the bucket (as well as new objects added in).
- Once a retention policy is locked:
  - You CANNOT remove retention policy or reduce retention period (You can increase the retention period). This is similar to the lock in.
  - You CANNOT delete the bucket unless all objects in the bucket have age greater than the retention period.

**Transferring data from on-premises to cloud storage**

- The end goal is to transfer data to Google Cloud Storage.
- Options for doing the transfer:
  - Online transfer: Use gsutil or API to transfer data to Google cloud storage.
    - This is suitable for one time transfer.
  - Storage Transfer Service: Recommended for large-scale (petabytes) online data transfers from your private data centers, AWS, Azure and Google Cloud.
    - You can setup a repeating schedule for the transfer.
    - Supports incremental transfers (only transfer changed objects)
    - Reliable and fault tolerant. (continues from where it left off if there is a failure)
  - Storage Transfer Service vs gsutil:
    - gsutil is recommended only when you are tranferring less than 1 TB from on-premises or another GCS bucket.
    - Storage Transfer Service is recommended if either of the conditions is met:
      - Transferring more than 1 TB of data from anywhere.
      - Transferring from another cloud.
- Migrating data with Transfer appliance:
  - Transfer appliance: Copy, ship and upload data to Google Cloud Storage.
    - It is only recommended when your data size is greater than 20TB. OR
      - Online transfer takes more than a week.
    - You can calculate this whole structure based on the speed of data transfer you can have.
    - Process:
      - Request an appliance
      - Upload your data into it.
      - Ship the appliance back to the Google data center.
      - Google will upload the data to cloud storage.
    - Fast copy (upto 40Gbps)
    - AES 256 encryption- Customer managed encryption keys.
    - Transfer appliance is available in different sizes (TA40, TA300)


**Best practises for Cloud Storage**

- Avoid use of sensitive data or info in bucket or object names.
- Store data in the closest region (to your users) to reduce latency.
- When using cloud, you need to ramp up your request rate gradually.
  - No problems upto 1000 write requests per second or 5000 read requests per second.
  - BUT beyond that, take at least 20 minutes to double request rates.
- Use exponential backoff if you receive 5xx (server error) or 429 (too many requests) errors.
  - Retry after 1,2,4,8,16,32,64,128,256,512,1024 seconds.
- Do not use sequential numbers or timestamp as object keys.
  - Recommended to use completely random object names
  - Recommended to add a hash value before the sequence number or timestamp.
- Use cloud storage FUSE to enable file system access to cloud storage.
  - Mount cloud storage buckets as file systems on your Linux and macOS machines.

  **Cloud Storage: Command Line: gsutil**

  - *gsutil* is a command line tool for interacting with Google Cloud Storage. (not the gcloud)
  - It is already installed in the Cloud Shell.
  - Cloud Storage(gsutil)
    - gsutil mb gs://bucket-name (create cloud storage bucket)
      - You need to set an active project for the cloud storage bucket to be created in.
    - gsutil ls -a gs://bucket-name (list all current and non-current objects in the bucket)
    - gsutil cp gs://SRC_BUCKET/SRC_OBJ gs://DEST_BUCKET/NAME_COPY (copy objects)
      - -o 'GSUtil:encryption_key=ENCRYPTION_KEY' (Encrypt Object)
    - gsutil mv (Rename/Move Objects)
      - gsutil mv gs://BKT_NAME/OLD_OBJ_NAME gs://BKT_NAME/NEW_OBJ_NAME
      - gsutil mv gs://OLD_BUCKET_NAME/OLD_OBJ_NAME gs://NEW_BUCKET_NAME/NEW_OBJ_NAME
    - gsutil rewrite -s STORAGE_CLASS gs://BKT_NAME/OBJ_NAME (Ex: Change storage class for objects)
    - gsutil cp: Upload and Download Objects
      - gsutil cp LOCAL_LOCATION gs://DESTINATION_BKT_NAME/ (Upload)
      - gsutil cp gs://SRC_BKT_NAME/OBJ_NAME LOCAL_LOCATION (Download)