**Google Cloud Operations**

**Cloud Monitoring**

- To operate cloud applications effectively, you must check for:
  - Application Health
  - Are the users experiencing any issues?
  - Does my databases have enough space?
  - Are my servers running in an optimal way?
- Cloud Monitoring: Tools to monitor your infrastructure
  - Measures key aspects of services (Metrics)
  - Create visualizations of metrics (Graphs and Dashboards)
  - Create alerts (when metrics are not healthy)
    - Define alerting policies: Specify the conditions that trigger an alert as through whcih channels you want to be notified and the documents which can specify the conditions that trigger an alert as well as through which channels you want to be notified and the documents which can be used to troubleshoot the issue.

**Cloud Monitoring: Workspace**

- You can use Cloud Monitoring to monitor one or more GCP projects and one or more AWS accounts.
- How do you group all the information from multiple GCP projects or AWS accounts?
- Create a Workspace
  - Workspace are needed to organize monitoring information
    - A workspace allows you to see monitoring information from multiple projects.
    - Step 1: Create workspace in a specific project (Host project)
    - Step 2: Add other projects to the workspace

**Cloud Monitoring: Virtual Machine Monitoring**

- Default monitoring metrics include:
  - CPU utilization
  - Disk utilization
  - Network utilization
  - Memory utilization
  - Disk read/write operations
  - Network read/write operations
- For more metrics its better to install Cloud Monitoring agent over the VM.
  - collectd-based daemon
  - Gather metrics from the VM and send them to Cloud Monitoring

**Cloud Logging**

- It's a real time log management and analysis tool.
- This allows you to store, search, analyze and alert on massive volume of data.
- Cloud logging is a exabyte scale, fully managed service.
  - No need of server provisioning, patching, etc
- Ingest log data from any source.
- Key Features:
  - Logs explorer- Search, sort and analyze using flexible queries.
  - Logs Dashboard- Rich Visualization
  - Logs Metrics- Capture metrics from logs (Using queries/matching strings)
  - Logs Router- Route different log entries to different destinations
- Most GCP managed services automatically send logs to cloud logging:
  - GKE
  - App Engine
  - Cloud Run
- Ingest logs from GCE VMs:
  - Install Logging Agent (based on fluentd)
  - (Recommended) Run logging agent on all VM instances
- Ingest logs from on-premises:
  - (Recommended) Use the BindPlane tool from Blue Medora
  - Use the cloud logging API

**Cloud Logging: Audit and Security Logs**

- Access Transparency Logs: Captures actions performed by GCP team on your content (Not supported by all services):
  - Only for organizations with Gold support level and above
- Cloud Audit Logs: Answers who did what, when and where:
  - Admin activity logs
  - Data access logs
  - System Event Audit logs
  - Policy denied Audit logs

![Audit logs](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Professional-Cloud-Architect_Certification-Exam-Preparation/blob/1e72c40c1a0893ac5fd2f3cef4ccbcbc4593cea9/Reference%20Images/audit%20logs.png "Cloud Logging: Audit and Security Logs")

- Which Service: protoPayload.serviceName
- Which Operation: protoPayload.methodName
- Which resource is audited: resource.type
- who is making the call: authenticationInfo.principalEmail

<table>
<thead>
<tr>
<th>Feature</th>
<th>Admin Activity Logs</th>
<th>Data Access Logs</th>
<th>System Event Logs</th>
<th>Policy Denied Logs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logs for</td>
<td>API calls or other actions that modify the configuration of resources</td>
<td>Reading configurations of resources</td>
<td>Google Cloud administrative actions</td>
<td>When user or Service account is denied access to a resource</td>
</tr>
<tr>
<td>Default Enabled</td>
<td>Yes</td>
<td>No (If you want to enable it, you can enable it for the specific service whose logs you want to see) for example you want to see cloud storage access logs you need to enable this for that specific bucket onto which you want to check the logs</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>VM Examples</td>
<td>VM Creation, Patching resources, Change in IAM permissions.</td>
<td>Listing Resources (VMs, Images etc)</td>
<td>On host maintenance, Instance preemption, Automatic Restart</td>
<td>Security Policy Violation logs</td>
</tr>
<tr>
<td>Cloud Storage</td>
<td>Modify Bucket or Object</td>
<td>Modify/Read Bucket or Object</td>
<td></td>
<td></td>
<tr>
<td>Access Needed (Required)</td>
<td>Logging/Logs Viewer or Project/Viewer</td>
<td>Logging/Private Logs Viewer or Project Owner</td>
<td>Logging/Logs Viewer or Project Viewer</td>
<td>Logging/Logs Viewer or Project Viewer</td>
</tr>
</tr>
</tbody>
</table>

**Cloud Logging: Controlling and Routing**

![Cloud Logging API](https://miro.medium.com/max/1400/1*2XrrxcdXq7hbiQX-6fGiBg.webp)

- How do you manage the logs?
  - Logs from various sources reaches Log Router
  - Log Router checks against configured rules
    - What to ingest? What to discard?
    - Where to route that log?
- Two types of logs buckets:
  - _Required: Holds admin activity, System Events and access transparency logs (retained for 400 days)
    - ZERO Charge
    - You cannot delete the bucket
    - You cannot change the retention period
  - _Default: Holds all other logs (retained for 30 days)
    - You are billed based on cloud logging pricing
    - You can delete the bucket
      - But you can disable the _Default log sink route to disable the ingestion!
    - You can also edit the retention settings (1 to 3650 days(10 years))

**Cloud Logging: Export**

- Logs are ideally stored in the cloud logging for limited period
  - For Long term retention (compliance, Audit) logs can be exported to:
    - Cloud Storage Bucket (ex:bucket/syslog/2025/05/05)
    - BigQuery dataset (ex: tables syslog_20250505 > columns timestamp, log)
    - Cloud Pub/Sub topic (base64 encoded log entries) If you want to send the logs to some other service like Splunk, ELK, etc you can use this option to send the logs to Pub/Sub topic and then you can use the Pub/Sub subscription to send the logs to the other service.
- How do you export logs?
  - Create sinks to these destinations using the log router

**Cloud Logging: Export: Use Cases**

- Use Case 1: Troubleshoot using VMs
  - Install cloud logging agent in all VMs and send logs to cloud logging
  - Search fpr logs in cloud logging
- Use Case 2: Export VM logs to bigquery for querying using SQL like queries
  - Install cloud logging agent in all VMs and Send logs to cloud logging
  - Create a BigQuery dataset for storing the logs
  - Create an export sink in Cloud logging with bigquery dataset as sink destination
- Use Case 3: You want to retain audit logs for external auditors at minimum cost
  - Create an export sink in cloud logging with cloud storage bucket as sink
  - Provide auditors with Storage Object Viewer role to the bucket
  - You can use the Google Data Studio role (for the visualization)