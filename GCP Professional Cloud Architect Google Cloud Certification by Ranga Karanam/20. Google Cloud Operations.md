**Google Cloud Operations**

**Cloud Monitoring**

- To operate cloud applications effectively, you must check for:
  - Application Health
  - Are the users experiencing any issues?
  - Does my databases have enough space?
  - Are my servers running in an optimal way?
- Cloud Monitoring: Tools to monitor your infrastructure
  - Measures key aspects of services (Metrics)
  - Create visualizations of metrics (Graphs and Dashboards)
  - Create alerts (when metrics are not healthy)
    - Define alerting policies: Specify the conditions that trigger an alert as through whcih channels you want to be notified and the documents which can specify the conditions that trigger an alert as well as through which channels you want to be notified and the documents which can be used to troubleshoot the issue.

**Cloud Monitoring: Workspace**

- You can use Cloud Monitoring to monitor one or more GCP projects and one or more AWS accounts.
- How do you group all the information from multiple GCP projects or AWS accounts?
- Create a Workspace
  - Workspace are needed to organize monitoring information
    - A workspace allows you to see monitoring information from multiple projects.
    - Step 1: Create workspace in a specific project (Host project)
    - Step 2: Add other projects to the workspace

**Cloud Monitoring: Virtual Machine Monitoring**

- Default monitoring metrics include:
  - CPU utilization
  - Disk utilization
  - Network utilization
  - Memory utilization
  - Disk read/write operations
  - Network read/write operations
- For more metrics its better to install Cloud Monitoring agent over the VM.
  - collectd-based daemon
  - Gather metrics from the VM and send them to Cloud Monitoring

**Cloud Logging**

- It's a real time log management and analysis tool.
- This allows you to store, search, analyze and alert on massive volume of data.
- Cloud logging is a exabyte scale, fully managed service.
  - No need of server provisioning, patching, etc
- Ingest log data from any source.
- Key Features:
  - Logs explorer- Search, sort and analyze using flexible queries.
  - Logs Dashboard- Rich Visualization
  - Logs Metrics- Capture metrics from logs (Using queries/matching strings)
  - Logs Router- Route different log entries to different destinations
- Most GCP managed services automatically send logs to cloud logging:
  - GKE
  - App Engine
  - Cloud Run
- Ingest logs from GCE VMs:
  - Install Logging Agent (based on fluentd)
  - (Recommended) Run logging agent on all VM instances
- Ingest logs from on-premises:
  - (Recommended) Use the BindPlane tool from Blue Medora
  - Use the cloud logging API

**Cloud Logging: Audit and Security Logs**

- Access Transparency Logs: Captures actions performed by GCP team on your content (Not supported by all services):
  - Only for organizations with Gold support level and above
- Cloud Audit Logs: Answers who did what, when and where:
  - Admin activity logs
  - Data access logs
  - System Event Audit logs
  - Policy denied Audit logs

![Audit logs](https://github.com/cloud-devops-enthusiast/Google-Cloud-Platform_Professional-Cloud-Architect_Certification-Exam-Preparation/blob/1e72c40c1a0893ac5fd2f3cef4ccbcbc4593cea9/Reference%20Images/audit%20logs.png "Cloud Logging: Audit and Security Logs")

- Which Service: protoPayload.serviceName
- Which Operation: protoPayload.methodName
- Which resource is audited: resource.type
- who is making the call: authenticationInfo.principalEmail

<table>
<thead>
<tr>
<th>Feature</th>
<th>Admin Activity Logs</th>
<th>Data Access Logs</th>
<th>System Event Logs</th>
<th>Policy Denied Logs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logs for</td>
<td>API calls or other actions that modify the configuration of resources</td>
<td>Reading configurations of resources</td>
<td>Google Cloud administrative actions</td>
<td>When user or Service account is denied access to a resource</td>
</tr>
<tr>
<td>Default Enabled</td>
<td>Yes</td>
<td>No (If you want to enable it, you can enable it for the specific service whose logs you want to see) for example you want to see cloud storage access logs you need to enable this for that specific bucket onto which you want to check the logs</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>VM Examples</td>
<td>VM Creation, Patching resources, Change in IAM permissions.</td>
<td>Listing Resources (VMs, Images etc)</td>
<td>On host maintenance, Instance preemption, Automatic Restart</td>
<td>Security Policy Violation logs</td>
</tr>
<tr>
<td>Cloud Storage</td>
<td>Modify Bucket or Object</td>
<td>Modify/Read Bucket or Object</td>
<td></td>
<td></td>
<tr>
<td>Access Needed (Required)</td>
<td>Logging/Logs Viewer or Project/Viewer</td>
<td>Logging/Private Logs Viewer or Project Owner</td>
<td>Logging/Logs Viewer or Project Viewer</td>
<td>Logging/Logs Viewer or Project Viewer</td>
</tr>
</tr>
</tbody>
</table>

**Cloud Logging: Controlling and Routing**

![Cloud Logging API](https://miro.medium.com/max/1400/1*2XrrxcdXq7hbiQX-6fGiBg.webp)

- How do you manage the logs?
  - Logs from various sources reaches Log Router
  - Log Router checks against configured rules
    - What to ingest? What to discard?
    - Where to route that log?
- Two types of logs buckets:
  - _Required: Holds admin activity, System Events and access transparency logs (retained for 400 days)
    - ZERO Charge
    - You cannot delete the bucket
    - You cannot change the retention period
  - _Default: Holds all other logs (retained for 30 days)
    - You are billed based on cloud logging pricing
    - You can delete the bucket
      - But you can disable the _Default log sink route to disable the ingestion!
    - You can also edit the retention settings (1 to 3650 days(10 years))

**Cloud Logging: Export**

- Logs are ideally stored in the cloud logging for limited period
  - For Long term retention (compliance, Audit) logs can be exported to:
    - Cloud Storage Bucket (ex:bucket/syslog/2025/05/05)
    - BigQuery dataset (ex: tables syslog_20250505 > columns timestamp, log)
    - Cloud Pub/Sub topic (base64 encoded log entries) If you want to send the logs to some other service like Splunk, ELK, etc you can use this option to send the logs to Pub/Sub topic and then you can use the Pub/Sub subscription to send the logs to the other service.
- How do you export logs?
  - Create sinks to these destinations using the log router

**Cloud Logging: Export: Use Cases**

- Use Case 1: Troubleshoot using VMs
  - Install cloud logging agent in all VMs and send logs to cloud logging
  - Search fpr logs in cloud logging
- Use Case 2: Export VM logs to bigquery for querying using SQL like queries
  - Install cloud logging agent in all VMs and Send logs to cloud logging
  - Create a BigQuery dataset for storing the logs
  - Create an export sink in Cloud logging with bigquery dataset as sink destination
- Use Case 3: You want to retain audit logs for external auditors at minimum cost
  - Create an export sink in cloud logging with cloud storage bucket as sink
  - Provide auditors with Storage Object Viewer role to the bucket
  - You can use the Google Data Studio role (for the visualization)

**Creating a Cloud Storage Bucket and Cloud Function**

- Create a cloud storage bucket
- Choose a option with "Process with Cloud Storage"
- This will take you to the cloud function creation page
- Give a name to the cloud function, select the region.
- Choose the trigger type as "Cloud Storage", event type as "Finalize/Create"
- Select the bucket you want to trigger the cloud function
- Select the runtime as Node.js 14
- Select the entry point as "helloGCS"
- Click on "Create" button
- This will create the cloud function and now you can upload the data to the cloud storage bucket and you can see the logs in the cloud function logs.
- Go to the bucket and upload some objects to the bucket and you can see the logs in the cloud function logs.
- You can filter these logs using the filter like resource.labels.function_name="my-bucket-trigger" and you can see the logs for the specific cloud function.
- These logs are stored in logs storage.
- There are two default buckets created in the project one is for the audit logs and the other one is for the default logs.
- The default logs come with 30 days of retention period and the audit logs come with 400 days of retention period.
- These logs are sent to here by the log router which is a service which is responsible for routing the logs to the specific destination.
- The logs different from the parameters defined in the logs router are sent to the default logs bucket.
- If you don't want to send the logs through these cloud routers you can disable the logs router.
- Creating a sink by giving name to sink, as destination you have multiple options like bigquery, cloud storage, pub/sub, splunk, etc.
- For instance we will send it to the cloud storage bucket.
- Now you can choose the option to choose the logs to be included in the sink by defining the conditions where you can define it as inclusion or exclusion filters.

- **Service like Cloud Functions, Cloud Run, App Engine, Google Kuberenetes Engine and Compute Engine are some of the usecases where you can use the cloud logging.**

**Playing with Cloud Monitoring**

- You can add the aws accounts and azure accounts to the cloud monitoring.
- You can see the resource dashboards here, and you can see the metrics for the resources.
- You can group the resources based on the labels.
- You can create and customize the dashboards based on your needs.
- You can set the alerts based on the metrics and configure the notifications by the channel.
- You can create the polcies based on the metrics and you can set the notifications based on the policies.
- You can set the trigger and you can set the notifications based on the trigger.
- You can set the uptime checks and you can set it basd on the targets like HTTP, HTTPS, TCP, etc over the multiple resource types. Here you can also configure the frequency of the checks. You can also go based on the custom and authetication headers.
- You can even set the response validation for the same with the settings like response content match type where you can choose between the match type like contains, equals, etc and you can also set the response content match string and the response content.
- In the last you can set an alert.

**Cloud Trace**

- Distributed tracing system for GCP: Collect latency data from:
  - Supported GCP services
  - Instrumented applications (Using tracing libraries) using the Cloud Trace API
- Finds out:
  - How long does a service take to handle requests?
  - What is the average latency of a request?
  - How are we doing over time? (Increasing/Decreasing trends)
- Supported for:
  - Compute Engine, GKE, App Engine (Flexible and Standard) etc.
- If you have applications you can use trace client libraries to instrument your application.
  - Supported Languages: Java, Python, Go, Node.js, Ruby, PHP, C#, C++, and .NET Core.

**Cloud Debugger**

- How to debug issues that are happening only in test or production environments?
  - Inspect the state of the applications directly in the GCP environment
  - Take snapshots of the vaiable and call stack
  - No need to add logging statements
  - No need to redeploy the application
  - It is very lightweight=>Very little impact to the users
    - Can be used in any environment: Test, Acceptance, Production

**Cloud Profiler**

- How do you identify performance bottlenecks in production?
- *Cloud Profiler* Statistical, Low-Overhead profiler
  - Continuosly gather CPU and memory usage from the production systems
  - Connect profiling data with application source code
    - Easily identify performance bottlenecks
  - Two major components:
    - Profiler agent: Collects Profiling Information (Collects CPU and memory usage data)
    - Profiler UI: Visualizes the data

**Error Reporting**

- How do you identify production problems in real time?
- Real time exception monitoring:
  - Aggregates and displays errors reported from the cloud services (Using the stack traces)
  - Centralized error Management console:
   - Identify and manage top errors or recent errors
  - Use firebase crash reporting for errors from android and IOS client applications
  - Error reporting is supported for GO, Java, Node.js, PHP, Python, Ruby, and .NET
- Errors can be reported by:
  - Sending them to Cloud logging or
  - By calling the error reporting API
- Error reporting tool can be accessed from desktop:
  - Also available in the cloud console mobile app for Android and iOS

**Stackdriver- Cloud Monitoring, Logging**

<table>
<thead>
<tr>
<th>Stackdriver Service</th>
<th>New Service Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stackdriver Monitoring</td>
<td>Cloud Monitoring</td>
</tr>
<tr>
<td>Stackdriver Logging</td>
<td>Cloud Logging</td>
</tr>
<tr>
<td>Stackdriver Error Logging</td>
<td>Error Reporting</td>
</tr>
<tr>
<td>Stackdriver Trace</td>
<td>Cloud Trace</td>
<tr>
<td>Stackdriver Profiler</td>
<td>Cloud Profiler</td>
</tr>
</tr>
</tbody>
</table>

**Cloud Operations: Scenarios**