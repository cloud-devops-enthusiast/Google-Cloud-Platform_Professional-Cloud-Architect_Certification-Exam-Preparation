**Google Kubernetes Engine (GKE)**

- Most popular container orchestration platform.
- Provides a cluster management (including upgardes).
  - Each cluster can have different types of virtual machines.
- Provides all important container orchestration features.
  - Auto Scaling
  - Service Discovery
  - Load Balancer
  - Self Healing
  - Zero Downtime Deployment
- This is a managed kubernetes service.
- Minimize operations with auto-repair(repair failed nodes) and auto-upgrade(use latest version of kubernetes) features.
- Provides pod and cluster autoscaling.
- Enable cloud logging and cloud monitoring with simple configuration.
- Uses container-optimized OS, a hardened os built by Google.
- Provides support for persistent disks and local SSDs.

**Kubernetes-A Microservice Journey**

1: Create a kubernetes cluster with the default node pool.
- Here you can use the command *gcloud container clusters create* or use cloud console for creating a cluster.
2: There are two modes of kubernetes clusters:
- Standard cluster: Kubernetes cluster with node configuration flexibility and pay-per-node.
- Autopilot Cluster: Optimized kubernetes cluster with a hands-off experience and pay-per-pod.
  - New mode of operation for GKE.
  - Reduce your operational costs in running Kubernetes clusters.
  - Provides an hands-off experience:
    - Do not worry about managing the cluster infrastructure(nodes, node pools..).
    - GKE will manage the clusters for you.

**Kubernetes Commands**

- For creating standard cluster:
  * Set the default project:
    - gcloud config set project {project-id}
  * Connect to the Kubernetes cluster
    - gcloud container clusters create get-credentials {cluster-name} -- zone us-central1-a --project {project-id}
  * Deploy microservice to kubernetes:
    * Create deployment and service using kubectl commands:
      - kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0.0.1.RELEASE
**kubectl is a kubernetes specific command, and this is a cloud neutral command.**
  * Check the deployment:
    - kubectl get deployment
  * Expose the deployment via the loadbalancer:
    - kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
  * Look at the status of the services:
    - kubectl get services
  * To continously monitor the status of the pods:
    - kubectl get services --watch
  * Sending curl or get request to the service:
    - curl 35.184.204.214:8080/hello-world
**You can add multiple node pools to your cluster.**
  * To see what is happening with your clusters:
    - kubectl get events
  * **Services**: Services are sets of pods with a network endpoint that can be used for discovery and load balancing.
  * **Ingress**: Ingresses are collections of rules for routing external traffic to services.
  * Increase number of instances for your microservice{Manual Scaling}:
    - kubectl scale deployment {deployment-name} --replicas=3
    - kubectl scale deployment hello-world-rest-api --replicas=3
  * Increase number of nodes in your kubernetes cluster( as we are running with 3 nodes, here we are scaling down to 2 nodes){Manual Scaling}:
    - gcloud container clusters resize my-cluster --node-pool default-pool --num-nodes=2 --zone=us-central1-c
  * Setup autoscaling for your cluster:
    - kubectl autoscale deployment hello-world-rest-api --max=4 --cpu-percent=70
  * HPA: Horizontal Pod Autoscaling:
    - kubectl get hpa
  * Setup autoscaling for your kubernetes cluster:
    - gcloud container clusters update {cluster-name} --enable-autoscaling --min-nodes=1 --max-nodes=3 --zone=us-central1-c
  * Add some application configuration for your microservice:
    - kubectl create configmap hello-world-config --from-literal=RDS_DB_NAME=todos
  * To get details inside of your configmap:
    - kubectl describe configmap hello-world-config
  * To check the configmap
    - kubectl get configmap
  * Add password configuration for your microservice{Kubernetes Secrets}:
    - kubectl create secret generic hello-world-secrets-1 --from-literal=RDS_PASSWORD=dummytodos
  * To get all the secrets inside your cluster:
    - kubectl get secret
  * To get details inside of your secret:
    - kubectl describe secret hello-world-secrets-1
  * If you want to edit the deployment and click on the edit button, you will be able to edit the deployment directly into the yaml file. Save the edited yaml file and you will be able to see the changes in the deployment.
  * To deploy the yaml file from the cloud console:
    - kubectl apply -f deployment.yaml
  * Deploy a new microservice which needs nodes with a GPU attached.
    * Attach a new node pool with the GPU instance to your cluster.
      - gcloud container node-pools create {Pool_Name} --cluster {Cluster_Name}
      - gcloud container node-pools list --cluster {Cluster_Name}
      - gcloud container node-pools list --zone=us-central1-c --cluster=my-cluster
  * Deploy the new microservice to the new pool by setting up *nodeselector* in the deployment.yaml file.
    - nodeSelector:cloud.google.com/gke-nodepool:{Pool_Name}
  - kubectl get pods -o wide
  - kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE
  - kubectl get services
  - kubectl get replicasets
  - kubectl get pods
  - kubectl delete pod hello-world-rest-api-58dc9d7fcc-8pv7r
  - kubectl scale deployment hello-world-rest-api --replicas=1
  - kubectl get replicasets
  - gcloud projects list
  * Delete the microservice:
    * Delete the service:  
      - kubectl delete service hello-world-rest-api
    * Delete the deployment:
      - kubectl delete deployment hello-world-rest-api
  * Delete the cluster:
    - gcloud container clusters delete my-cluster --zone us-central1-c


* **Google Kubernetes Engine(GKE) Cluster**

- As starting off with kubernetes, you will need to create a cluster.
- *Cluster* is a set of compute engine instances that run containerized applications. There are mainly two types of nodes inside our clusters:
  - Master Node(s): Manages the cluster.
  - Worker Node(s): Runs your workloads(pods).

![Cluster](https://www.suse.com/c/wp-content/uploads/2019/04/What-is-Cluster_-1024x309.jpg "What is cluster")

![Master Node](https://www.suse.com/c/wp-content/uploads/2019/04/What-is-Master-Node-in-K8s_-1024x611.jpg "Master Node in Kuberenetes")

- *Master Node* (Control plane) components:
  - API Server: Handles all the communication for a k8s cluster (from nodes and outside the cluster).
  - Scheduler: Decides placement of pods.
  - Control Manager: Manages deployments and replica sets.
  - etcd: Distribute database storing the cluster state.
- *Worker Node* (Node) components:
  - Runs your pods where your workloads are running.
  - Kubelet: This manages the communication with master node(s). It is used to talk with the master node(s) and to run the pods.

- GKE Cluster Types:
  - **Zonal Cluster**:
    - Single Zone- Single control plane. Nodes running in the same zone.
    - Multi Zone- Single control plane. Nodes running in multiple zones.
  - **Regional Cluster**:
    - Replicas of the control plane runs in multiple zones of a given region. Nodes also run in same zones where control plane runs.
    - Master nodes are spread across multiple zones in a region even the master nodes can fail, if there is a failure in one zone, the master nodes in other zones will take over.
  - **Private Cluster**:
    - VPC network cluster. Nodes only have internal IP addresses. No public IP addresses. These clusters are not accessible from the internet and just stay in your VPC network.
  - **Alpha Cluster**:
    - Clusters with alpha APIs- early feature APIs. These are used to test new kubernetes features.

* **Kubernetes- Pods**

- Pods are the smallest deployable units in kubernetes.
- A pod contains one or more containers.
- Each pod is assigned an ephemeral IP address.
- Pods are the place where your microservice runs.
- If you have multiple containers in a pod, they will share:
  - Network
  - Storage
  - IP Address
  - Ports and
  - Volumes (Shared persistent disks)
- POD statuses:
  - Running: When pod is running.
  - Pending: When pod is waiting to be deployed to one of the nodes.
  - Succeeded: When pod has completed successfully.
  - Failed: When pod was not started successfully.
  - Unknown: When master is unable to find status of the pod.

* **Kubernetes- Deployments vs Replica Set**

- A deployment is created for each microservice.
  - kubectl create deployment m1 --image=m1:v1
  - Deployment represents a microservice (with all its releases).
  - Deployment manages new releases ensuring zero downtime.
    - kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0.0.2.RELEASE
- A Replica Set ensures that a specific number of pods are running for a specific microservice version. Even if you delete any pod using kubectl delete pod, the replica set will create a new pod as per the configuration of the replica set.
  - kubectl get replicasets : This command will return the deployments and the number of pods running for each deployment with their age.
  - kubectl scale deployment hello-world-rest-api --replicas=1 : This command will scale down the number of pods running for the deployment to 1.
  - After running the above command, you will see that the replica set will create a new pod to maintain the number of pods running for the deployment to 1.
  - Even if one of the pods is killed, replica set will launch a new one.
- Deploy V2 of microservice- Creates a new replica set
  - kubectl set image deployment m1 m1=m1:v2
  - V2 replica set is created
  - Deployment updates V1 replica set and V2 replica set based on the release strategies.

* **Kubernetes- Services**

- Each pod has its own ip address.
  - How do you ensure that the external users are not impacted here:
    - A pod fails and is replicated by a replica set.
    - A new release happens and all existing pods of the old release are replaced by ones of the new release.
  - Create Service:
    - kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080
      - Expose PODs to outside world using a stable IP address.
      - Ensure that the external world does not get impacted as pods go down and come up.
- Three types of services:
  - ClusterIP: Exposes service on a cluster-internal IP.
    - Use Case: You want your microservice only to be available inside the cluster (Intra Cluster Communication).
  - Load Balancer: Exposes service externally using a cloud provider’s load balancer.
    - Use Case: You want to create individual Load Balancers for each microservice.
  - NodePort: Exposes service on each Node's IP at a static port (the NodePort).
    - Use Case: You do not want to create an external load balancer for each microservice. (You can create one Ingress component to load balance multiple microservices.)

* **Kubernetes- Ingress**

- In simple language, Ingress is a set of rules that allow external traffic to reach your services through different ports and service name. Its like a map to every service in your cluster with ports as doors to every service.
- Kubernetes Ingress are collections of rules for routing external HTTP(s) traffic to the services. 
- Ingress is the recommended approach for providing external access to services in a cluster.
  - Provides load balancing and SSL termination.
  - Control traffic by defining rules on the ingress resource.
  - Recommendation: Node port service for each microservice. Expose using a Ingress.
    - Advantage: One load balancer can serve multiple microservices.

  Code:

  apiVersion: networking.k8s.io/v1 : This is the api version of the ingress resource.

  kind: Ingress : This is the kind of the ingress resource.

  metadata: This is the metadata section of the ingress resource.

  name: This is the name of the ingress resource.

  spec: This is the spec section of the ingress resource.

    rules: This is the rules section of the ingress resource.

      http: This is the http section of the ingress resource.

        paths: This is the paths section of the ingress resource.

        path: This is the path section of the ingress resource.

            backend: This is the backend section of the ingress resource.

              service: This is the service section of the ingress resource.

                - name: This is the name of the service.

                - port: This is the port of the service.


* **Container Registry - Image Repository**

- This is a private repository by default.
- As you have created a docker image, you need a place to store it. Here comes the role of container registry.
- Container Registry is a fully managed container registry provided by GCP.
- This is an alternative to Docker Hub.
- This can be integrated to CI/CD tools to publish images to the registry.
- You can secure your container images. Analyse for vulnerabilities and enforce deployment policies.
- Naming: HostName/ProjectID/Image:Tag-gcr.io/projectname/helloworld:1

* **Understanding Best Practices for Creating Docker Images - Dockerfile**

- Whenever you create a docker image, you need to create a docker file.
- Docker file contains the instructions to create a docker image.
- Keep the structure of the docker file simple.
- File Descripters:
  - FROM- Sets a base image
  - WORKDIR- Sets the working directory
  - RUN - Execute a command
  - EXPOSE- Inform docker about the port that the container listens on at runtime.
  - COPY- Copies new files or directories into image.
  - CMD- Default command for an executing container.
- Use light weight images(Prefer alpine images over ubuntu images).
- Do not copy anything that is unnecessary (node_modules for example).
- Move things that change less often or frequently to the top to enable LAYER REUSE. As the things doesnt change it will keep reusing the same layer. This will reduce the time required to build the image.
  - Code changes often (index.js) but dependencies change less often (package.json contains libraries that the app needs).

*Code:*

  FROM node:8.16.1-alpine   //Base Image

  WORKDIR /app

  COPY . /app

  RUN npm install

  Expose 5000

  CMD node index.js

* **GKE Scenarios**

- Scenario 1: You want to keep your costs low and optimize your GKE implementation.
  - Solution: Consider preemptible VMs, Appropriate region, Committed use discounts. E2 machine types are cheaper than N1 machine types. Choosing the right environment to fit your workload type(Use multiple node pools if needed, where you can have a node pool with gpu attached to make use of high power compute).
- Scenario 2: You want an efficient, completely auto scaling GKE solution.
  - Solution: Configure Horizontal Pod Autoscaler, Autoscaler for deployments and Cluster Autoscaler for node pools.
- Scenario 3: You want to execute untrusted third party code in Kubernetes Cluster.
  - Solution: Create a new node pool with GKE sandbox. Deploy untrosted code to the sandbox node pool.
- Scenario 4: You want to enable only internal communication between your microservice deployments in a kubernetes cluster.
  - Solution: Create service of type ClusterIP
- Scenario 5: My pod stays pending.
  - Solution: Most probably pods cannot be scheduled onto a node(insufficient resources). In this case you can increase the number of nodes in the node pool.
- Scenario 6: My pod stays waiting.
  - Solution: Most probably failure to pull the image. It can be also happen due to insufficient rights to pull the image. In this case you can check the image name and tag. Also check the IAM roles.

* **Delete GKE Service, Deployment and Cluster**

- Delete Service: kubectl delete service hello-world-rest-api
- Delete Deployment: kubectl delete deployment hello-world-rest-api
- Delete Cluster: gcloud container clusters delete hello-world-rest-api --zone us-central1-a (you need to set the project)

* *Kuberenetes terminology*

- HARDWARE(Cluster)
  - Master Node(s)- Manage the cluster.
  - Worker Node(s)- Run the workloads(as pods).
  - Node Pool- Group of nodes in a cluster with same configuration.
- Software
  - Pods- Smallest deployable unit in kubernetes. Deployed to the worker nodes.
  - Deployments- Manage pods.
  - Services- Expose the deployments to the outside world.

Notes:

- GKE is the GCP service which can be used to deploy a complex microservice architecture using container orchestration.
- ReplicaSet can be used to identify and replace the pods when a pod becomes unhealthy.
- Secrets are used to store sensitive information like configuration (passwords, tokens, etc).
- *gcloud container clusters get-credentials* command is used to connect to the kubernetes cluster.
- Update the existing deployment with the image of a new version of the microservice for deploying a new version of the microservice without downtime.
- **Q:** You have 3 instances of a microservice deployed to a kubernetes cluster. you want to enable autoscaling for the microservice. You also want to automatically increase the number of nodes in the cluster if existing nodes are not sufficient . Which approach you will use?
  - **A:** Use *kubectl autoscale deployment* command to auto-scale pods and use *gcloud container clusters update* command with --enable-autoscaling to auto-scale cluster.