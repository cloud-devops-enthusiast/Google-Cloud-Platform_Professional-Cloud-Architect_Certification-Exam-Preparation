**Google Cloud Database Explorations**

**Database Primer**

- Database provide organized and persistent storage of data.
- To choose between multiple different data types, we need to understand the need of database.
  - Availability
  - Durability
  - RTO
  - RPO
  - Consistency
  - Transactions

**Database- Getting Started**

- Imagine a database deployed in a datacenter in london.
- Challenges with databases:
  - Challenge 1: Your database will go down if the datacenter crashes or the server storage fails.
  - Challenge 2: You will lose data if the database crashes.
- That's why you can take *Database: Snapshots*
- We can automate taking copy of the database (taking a snapshot) every hour to another data center in region.
- Challenges after configuring Snapshots:
  - Challenge 1: Your database will go down if the data center crashes.
  - Challenge 2 (Partially Solved): You will lose data if the database crashes.
    - You can setup database from latest snapshot. But depending on when failure occurs you can lose up to an hour of data.
- Database Transaction Logs:
  - Let's add transaction logs to database and create a process to copy it over to another data center.
  - Challenges after configuring Transaction Logs:
    - Challenge 1: Your database will go down if the data center crashes.
    - Challenge 2 (Solved): You will lose data if the database crashes.
      - You can setup database from latest snapshot and apply transaction logs to it.
    - Challenge 3: Database will be slow when you take snapshots, as the process will be busy copying data.
- Database- Add a Standby
  - Adding this standby database will be in the second data center with replication.
  - Challenges after configuring Standby:
    - Challenge 1 (Solved): Your database will go down if the data center crashes.
      - You can switch to the standby database.
    - Challenge 2 (Solved): You will lose data if the database crashes.
    - Challenge 3 (Solved): Database will be slow when you take snapshots.
      - Take snapshots from standby database.
      - Application connecting to master will get good performance always.

**Database Availability and Durability**

- Availability
  - Will I be able to access my data now and when I need it?
  - Percentage of time an application provides the operations expected of it.
- Durability
  - Will my data be available after 10 or 100 or 1000 years?
- Examples of measuring availability:
  - 4 9's (99.99%)
  - 11 9's (99.999999999%)
- Typically, an availability of four 9's is considered very good.
- Typically, a durability of eleven 9's is considered very good.

<table>
<thead>
<tr>
<th>Availability</th>
<th>Downtime(In a Month)</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>99.95%</td>
<td>22 Minutes</td>
<td>-</td>
</tr>
<tr>
<td>99.99% (4 9's)</td>
<td>4 and 1/2 minutes</td>
<td>Typically online appps aim for 99.99% (4 9's) availability.</td>
</tr>
<tr>
<td>99.999% (5 9's)</td>
<td>26 Seconds</td>
<td>Achieving 5 9's availability is tough</td>
</tr>
</tbody>
</table>

- Durability
  - What does a durability of 11 9's mean?
    - If you store one million files for ten million years, you would expect to lose only one file.
  - Database durability should be high because the data loss is a a very big concern.
    - Once the data is gone, it is gone forever.

**Increasing Availability and Durability of Databases**

- Increase Availability
  - Have multiple standbys available OR distribute the database.
    - in Multiple Zones
    - in Multiple Regions
- Increase Durability
  - Multiple copies of data (standbys, snapshots, transaction logs and replicas).
    - In Multiple Zones
    - In Multiple Regions
- Replicating data comes with its own challenges.

**Database Terminology: RTO and RPO**

- Consider some scenarios:
  - Imagine a financial transaction being lost.
  - Imagine a trade being lost.
  - Imagine a stock exchange going down for an hour.
- Typically bussinesses are fine with some downtime but they all hate losing their data.
- Availability and Durability are the technical measures.
- RPO (Recovery Point Objective): Maximum acceptable period of data loss
- RTO (Recovery Time Objective): Maximum acceptable period of downtime
- Achieving both RPO and RTO is expensive.
- Trade-off based on the criticality of the data.
- **QUESTION**: You are running an application in VM instance storing it's data on a persistent data storage. You are taking snapshots every 48 hours. If the VM instance crashes, you can manually bring it backup in 45 minutes from the snapshot. What is your RTO and RPO?
  - RTO: 45 minutes
  - RPO: 48 hours
- Achieving RTO and RPO- Failover Examples:

<table>
<thead>
<tr>
<th>Scenario</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Very Small Data Loss(RPO-1 Minute) and Very Small downtime(RTO-5 Minute)</td>
<td>Hot StandBy- Automatically synchronize data. Have a standby ready to pickup load. Use automatic failover from master to standby.</td>
</tr>
<tr>
<td>Very Small Data loss(RPO-1 Minute) BUT here we can tolerate some downtimes(RTO-15 Minutes)</td>
<td>Warm StandBy- Automatically synchronize data. Have a standby with minimum infrastructure. Scale it up whenever a failure happens.</td>
</tr>
<tr>
<td>Data is critical(RPO-1 Minute) but I can tolerate downtime of a few hours(RTO-Few Hours)</td>
<td>Create regular data snapshots and transaction logs. Create database from snapshots and transaction logs when a failure happens.</td>
</tr>
<tr>
<td>Data can be lost without a problem (For example: Cached Data)</td>
<td>Failover to a completely new server.</td>
</tr>
</tbody>
</table>

**(NEW SCENARIO) REPORTING AND ANALYTICS Applications**

- These new reporting and analytics applications are being launched using the same databases.
  - These applications will only read data.
- Within a few days you see that the database performance is impacted.
- Here we can fix this:
  - Vertically Scale the database: Increase the CPU and Memory
  - Create a database cluster (Distribute the database): Typically database clusters are expensive to setup.
  - Create read replicas: Run read only applications against read and replicas.
- **Database~Read Replicas**
  - Add read replicas to the database.
  - Connect reporting and analytics applications to read replicas.
  - This reduces the load on the master database.
  - Upgrade read replica to master database (Supported by some databases).
  - Create read replicas in multiple regions.
  - Take snapshots of read replicas.

**Database- Consistency**

- This is the process how we ensure that data in multiple database instances (standbys and replicas) is updated simultaneously.
- *Strong Consistency*- Synchronous replication to all replicas
  - Databases will be slow in case you have multiple replicas and standbys.
- *Eventual Consistency*- Asynchronous replication happens here followed by a little lag- few seconds- before the change is available in all replicas.
  - In the intermediate period of time, different replicas might return different values.
  - Used when scalability is more important than data integrity.
  - Examples: Social media posts- Facebook status messages, twitter tweets, linked in posts, etc.
- *Read-after-Write Consistency*- Inserts are immediately available.
  - However, updates would have eventual consistency.

**Understanding Database Fundamentals: Choosing the Right Database**

- Database Categories:
  - Relational (OLTP and OLAP), Document, Key-Value, Graph, In memory among others.
- Choosing the type of database for your use case is not easy. A few factors: 
  - Do you want a fixed schema?
    - Do you want flexibility in defining and changing your schema? (Schemaless)
  - What level of transaction properties do you need? (Atomicity and Consistency)
  - What kind of latency do you want? (Seconds, milliseconds or microseconds)
  - How many transactions do you expect? (Hundreds or thousands or millions of transaction per second)
  - How much data will be stored? (MBs or GBs or TBs or PBs)
  - and a lot more factors.

- *Relational Databases*
  - This was the only option until a decade back.
  - Most popular (or unpopular) type of database.
  - Predefined schema with tables and relationships.
  - Primary key is a unique identifier for the database, for example, ID in a table.
    - Example: ID(int), Name(varchar), Age(int), Address(varchar)
  - Very strong transaction capabilities.
  - This is used for 
    - OLTP (Online Transaction Processing) Use Cases (Example Bank or Stock trading applications)
    - OLAP (Online Analytical Processing) Use Cases (Example: Reporting and Analytics)

- *Relational Database- OLTP(Online Transaction Processing)*
  - Applicationn where large number of users make large number of small transactions.
    - Small data reads, updates and deletes
  - Use cases:
    - Most traditional applications, ERP, CRM, e-commerce, banking applications
  - Popular Relational Databases:
    - Oracle, MySQL, SQL Server, etc.
  - **Recommended Google Managed Service**:
    - Cloud SQL: Supports PostgreSQL, MySQL, and SQL Server for regional relational databases (upto a few TBs of data).
    - Cloud Spanner: Unlimited scale (multiple PBs) and 99.999% availability for global applications with horizontal scaling.

- *Relational Database- OLAP(Online Analytical Processing)*
  - Applications allowing users to analyze petabytes of data.
    - Use cases:
      - Reporting and Analytics
      - Data Warehousing
      - Business Intelligence Applications
      - Analytics Applications
    - Sample Applications: Decide insurance premiums analyzing data from the last hundered years.
    - Data is consolidated from multiple transactional databases.
  - Recommended GCP Managed service:
    - BigQuery: Petabyte Scale distribution data ware house.

- *Relational Database- OLTP vs OLAP*
  - OLAP and OLTP use similar data structures.
  - But very different approach in how data is stored.
  - OLTP databases use row storages:
    - Each table row is stored together
    - Efficient for processing small transactions
  - OLAP databases use column storage:
    - Each table column is stored together
    - High Comparison: Store petabytes of data efficiently
    - Distribute data: One table in multiple cluster nodes
    - Execute Single query across multiple nodes- Complex queries can be executed efficiently.

- *NoSQL Database in GCP- Firestore, Datastore and Bigtable*
  - New approach to building your databases.
    - NoSQL= not only SQL
    - Flexible schema
      - Structure data the way your application needs it
      - Let the schema evolve with the time,.
    - Horizontally Scale to petabytes of data with millions of tps(transaction per second).
  - NOT a 100% accurate generalization but a good starting point.
    - Typical NoSQL databases trade-off "Strong Consistency and SQL Features" to achieve "Scalability and High Performance".
  - Google managed services for NO-Sql option:
    - Cloud Firestore (Datastore)
    - Cloud Bigtable

- Cloud Firestore (Datastore) vs Cloud Bigtable
  - Cloud Datastore: Managed serverless NoSQL document database.(Small DBs)
    - This provides you with ACID transactions, SQL-like queries, Indexes.
      - Designed for transactional mobile and web applications.
  - FireStore(Next version of Datastore) adds:
    - Strong Consistency
    - Mobile and Web client libraries
  - Recommended for small to medium size databases (0 to a few TBs of data).
  - Cloud Bigtable: Managed, Scalable NoSQL wide column database. (Large DBs)
    - NOT serverless (You need to create instances)
    - Recommended for data size > 10 Terabytes to several petabytes.
    - Recommended for large analytical and operational workloads.
      - Not recommended for transactional workloads. (Does not support multi-row transactions- Supports only single row transactions)

- In-Memory Databases
  - Retriving data from memory is much faster than retrieving data from disk.
  - In-memory databases like REDIS deliver microsecond latency by storing persistent data in memory.
  - Recommended GCP managed service:
    - MemoryStore
  - Use cases: Caching, Session management, Leaderboards, Real-time analytics, Geospatial data, etc.

**Databases Summary**