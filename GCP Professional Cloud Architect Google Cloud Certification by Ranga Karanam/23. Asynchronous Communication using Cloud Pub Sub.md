**Decoupling applications using Cloud Pub/Sub**

- Need for Asychronous Communication:

- **Synchronous Communication:**

  - **Web Server** -> **Logging Service** -> **Database**

  - Applications on your web server make synchronous calls to the logging service.
  - What if your logging service goes down?
    - Will you applications go down too?
  - What if all of sudden, there is high load and there are lot of logs coming in.

- **Asynchronous Communication:**

  - **Web Server** -> **Topic** -> **Logging Service** -> **Database**

  - Create a topic and have your applications put log messages on the topic.
  - Logging service picks them up for processing when ready.
  - Advantages:
    - Decoupling: Publisher (Apps) don't care about who is listening
    - Availability: Publisher (Apps) up even if a subscriber (Logging Service) is down
    - Scalability: Scale consumer instances (Logging Service) under high load
    - Durability: Message is not lost even if subscriber (Logging Service) is down

**Pub/Sub**

- Reliable, Scalable, fully-managed asynchronous messaging service
- Backbone for Highly available and highly scalable solutions
  - Auto scale to process billions of messages per day
  - Low cost (pay for what you use)
- Usecases: Event ingestion and delivery for streaming analytics pipelines.
- Support push and pull message deliveries

**Pub/Sub - How does it work?**

- First create a topic
- To which multiple publishers can publish messages
- *Publisher*- Sender of a message
   - Publisher send messages by making HTTPS requests to pubsub.googleapis.com
- *Subscriber*- Receiver of a message
   - Pull- Subscriber pulls messages when ready
     - Subscriber makes HTTPS requests to pubsub.googleapis.com
   - Push- Messages are sent to subscribers
     - Subscribers provide a web hook endpoint at the time of regression
     - When a message is received on the topic, A HTTPS POST request is sent to the web hook endpoint
- Very Flexible Publishers and Subscribers Relationships: One to Many, Many to Many, Many to One

**Pub/Sub - Getting ready with Topics and Subscriptions**

- Step 1: Create a topic
- Step 2: Create a subscription
  - Subcribers are registered to a topic
  - Each subscription represents discrete pull of messages from a topic
    - Multiple clients pull same subscription => Messages split between clients
    - Multiple clients create a subscription each = > each client will get every message

**Pub/Sub - Sending and Receiving Messages**

- Publisher sends message to a topic
- Message individualy delivered to each and every subscription
  - Subscriber can receive message either by:
    - Push: Pub/Sub sends the message to subscriber
    - Pull: Subscriber poll for messages
- Subscriber sends the acknowledgements
  - Pub/Sub ensures that the message is retained per subscription until it is acknowledged

**Commands:**

- gcloud config set project [PROJECT_ID]
- gcloud pubsub topics create topic-from-gcloud
- gcloud pubsub subscription create subscription-gcloud-1 --topic=topic-from-gcloud
- gcloud pubsub subscription create subscription-gcloud-2 --topic=topic-from-gcloud
- gcloud pubsub subscriptions pull subscription-gcloud-1
- gcloud pubsub subscriptions pull subscription-gcloud-2
- gcloud pubsub topics publish topic-from-gcloud --message="Hello from gcloud"
- gcloud pubsub topics publish topic-from-gcloud --message="Hello from gcloud 2"
- gcloud pubsub topics publish topic-from-gcloud --message="Hello from gcloud 3"
- gcloud pubsub subscriptions pull subscription-gcloud-1 --auto-ack
- gcloud pubsub subscriptions pull subscription-gcloud-2 --auto-ack
- gcloud pubsub topics list
- gcloud pubsub topics delete topic-from-gcloud
- gcloud pubsub topics list-subscriptions topic-from-gcloud

**Creating a Topic and Subscription**

- Create a topic
- You have option to export to bigquery or cloud storage
- Here in subscription, you can only see the subscriptions attached to that particular topic. A Subscription captures the stream of messages published to a given topic. You can also stream messages to bigquery or cloud storage by creating a subscription from a cloud dataflow job. A subscription directs messages on a topic to the subscribers. Messages can be pushed to subcriber immediately or subcriber can pull messages as needed.
- Here you can choose the delivery type as either push or pull type.
- Here as you choose the subscription id will be added into your topic.
- You can set the message retention duration, the unacknowledged messages will be retained whereas if retain acknowledged messages is enabled, the acknowledged messages will be retained for the same duration.
- You can set the acknowledgement deadline as how long Pub/Sub will wait for the subscriber to acknowledge receipt before resending the message.
- Here on the same screen you can specify the filter you will only receive messages that matches the filter.
- Here now you can create the topic.

**Understanding Cloud Pub/Sub Best Practices**

- UseCase:
  - Convert synchronous to asynchronous workflows.
    - Also useful when consumer is unable to keep up with the producer (buffer data). Whenever a consumer is unable to keep up with the producer, cloud pub/sub can come up with loose coupling to buffer the data.
    - The popular alternatives to the same are RabbitMQ, Apache Kafka, etc.
  - Apply transformations to IOT data streams
    - IOT data streams can be transformed using cloud pub/sub.
  - Some Usecases need in-order, exactly-one processing (de-duplication of messages)
    - Pub Sub supports in order processing.
      - Option --enable-message-ordering on subscription
    - If you want to enable message de-duplication, you can add Dataflow into flow for the same. (Exactly once processing)
      - Maintains a list of message IDs for a time period.
      - If a messsage ID repeats, it is discarded (assumed to be a duplicate)

**Cloud Dataflow**

- Dataflow provides unified streaming and batch data processing that's serverless, fast and cost-effective.
- You have option to choose from multiple dataflow templates for creating a dataflow job.
- You can choose the regional endpoints.
- You can have a look over some few example pipelines you can build:
  - Pub/Sub > Dataflow > BigQuery (Streaming)
  - Pub/Sub > Dataflow > Cloud Storage (Streaming-files)
  - Cloud Storage > Dataflow > BigTable/CloudSpanner/Datastore/BigQuery (Batch Load data into the databases.)
  - Bulk compression of cloud storage files (Batch)
  - Convert files from one format to another (Batch) like avro, parquet and csv.